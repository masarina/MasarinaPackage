import torch
from torch import nn, optim

class PytorchDebuger:
    def __init__(self):
        pass

    @staticmethod
    def compare_model_optimizer_shapes(model, optimizer):
        """
        ãƒ¢ãƒ‡ãƒ«ã®å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®é‡ã¿ã€ãƒã‚¤ã‚¢ã‚¹ã€ãƒ™ãƒ¼ã‚¿å€¤ã®shapeã‚’Optimizerã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨æ¯”è¼ƒã—ã€
        ä¸ä¸€è‡´ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®åå‰ã¨shapeã‚’è¡¨ç¤ºã™ã‚‹é–¢æ•°ã€‚
        """
        # ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
        model_params = {name: param.shape for name, param in model.named_parameters()}

        # Optimizerã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
        optimizer_params = {f"optimizer_{i}": p.shape for i, p in enumerate(optimizer.param_groups[0]['params'])}

        # æ¯”è¼ƒçµæœã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
        mismatched_layers = []

        print("ã€Shapeæ¯”è¼ƒçµæœã€‘")
        print("-" * 50)

        # ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨Optimizerã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¯”è¼ƒ
        for name, model_shape in model_params.items():
            # Optimizerãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨Shapeã‚’æ¯”è¼ƒ
            match_found = any(model_shape == opt_shape for opt_shape in optimizer_params.values())

            if not match_found:
                mismatched_layers.append((name, model_shape))
                print(f"ä¸ä¸€è‡´ãƒ¬ã‚¤ãƒ¤ãƒ¼: {name}, Shape: {model_shape}")

        if not mismatched_layers:
            print("å…¨ã¦ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ShapeãŒOptimizerã«ä¸€è‡´ã—ã¦ã„ã¾ã™ï¼ğŸ‰")
        else:
            print("\nã€ä¸ä¸€è‡´ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ä¸€è¦§ã€‘")
            for layer_name, shape in mismatched_layers:
                print(f"ãƒ¬ã‚¤ãƒ¤ãƒ¼å: {layer_name}, Shape: {shape}")

    @staticmethod
    def compare_optimizers(prev_optimizer, current_optimizer):
        """
        å‰å›ã®Optimizerã¨ä»Šå›ã®Optimizerã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¯”è¼ƒã—ã€ä¸ä¸€è‡´ãŒã‚ã‚‹å ´åˆã«è¡¨ç¤ºã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰ã€‚

        Args:
            prev_optimizer (torch.optim.Optimizer): å‰å›ã®Optimizer
            current_optimizer (torch.optim.Optimizer): ä»Šå›ã®Optimizer
        """
        # å‰å›ã¨ä»Šå›ã®Optimizerã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
        prev_params = [p.shape for group in prev_optimizer.param_groups for p in group['params']]
        current_params = [p.shape for group in current_optimizer.param_groups for p in group['params']]

        print("ã€Optimizerãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¯”è¼ƒçµæœã€‘")
        print("-" * 50)

        # æ¯”è¼ƒ
        if prev_params == current_params:
            print("å‰å›ã¨ä»Šå›ã®Optimizerã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ä¸€è‡´ã—ã¦ã„ã¾ã™ï¼ğŸ‰")
        else:
            print("ä¸ä¸€è‡´ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã™ï¼")
            print(f"å‰å›ã®Optimizerã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {len(prev_params)}")
            print(f"ä»Šå›ã®Optimizerã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {len(current_params)}")

            # Shapeã®é•ã„ã‚’ç¢ºèª
            for i, (prev_shape, curr_shape) in enumerate(zip(prev_params, current_params)):
                if prev_shape != curr_shape:
                    print(f"ä¸ä¸€è‡´ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {i+1}, å‰å›: {prev_shape}, ä»Šå›: {curr_shape}")
