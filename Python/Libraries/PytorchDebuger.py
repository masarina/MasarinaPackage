import torch
from torch import nn, optim

class PytorchDebuger:
    def __init__(self):
        pass

    @staticmethod
    def compare_model_optimizer_shapes(model, optimizer):
        """
        ãƒ¢ãƒ‡ãƒ«ã®å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®shapeã‚’Optimizerã¨æ¯”è¼ƒã—ã€ä¸ä¸€è‡´ã‚’è¡¨ç¤ºã™ã‚‹ã€‚
        """
        model_params = {name: param.shape for name, param in model.named_parameters()}
        optimizer_params = {f"optimizer_{i}": p.shape for i, p in enumerate(optimizer.param_groups[0]['params'])}

        mismatched_layers = []

        print("ã€Shapeæ¯”è¼ƒçµæœã€‘")
        print("-" * 50)

        for name, model_shape in model_params.items():
            match_found = any(model_shape == opt_shape for opt_shape in optimizer_params.values())
            if not match_found:
                mismatched_layers.append((name, model_shape))
                print(f"ä¸ä¸€è‡´ãƒ¬ã‚¤ãƒ¤ãƒ¼: {name}, Shape: {model_shape}")

        if not mismatched_layers:
            print("å…¨ã¦ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ShapeãŒOptimizerã«ä¸€è‡´ã—ã¦ã„ã¾ã™ï¼ğŸ‰")
        else:
            print("\nã€ä¸ä¸€è‡´ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ä¸€è¦§ã€‘")
            for layer_name, shape in mismatched_layers:
                print(f"ãƒ¬ã‚¤ãƒ¤ãƒ¼å: {layer_name}, Shape: {shape}")

    @staticmethod
    def compare_optimizer_optimizer_shapes(prev_optimizer, current_optimizer):
        """
        å‰å›ã¨ä»Šå›ã®Optimizerã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿shapeã‚’æ¯”è¼ƒã—ã€ä¸ä¸€è‡´ã‚’è¡¨ç¤ºã™ã‚‹ã€‚
        """
        prev_params = [p.shape for group in prev_optimizer.param_groups for p in group['params']]
        current_params = [p.shape for group in current_optimizer.param_groups for p in group['params']]

        print("ã€Optimizerãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¯”è¼ƒçµæœã€‘")
        print("-" * 50)

        if prev_params == current_params:
            print("å‰å›ã¨ä»Šå›ã®Optimizerã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ä¸€è‡´ã—ã¦ã„ã¾ã™ï¼ğŸ‰")
        else:
            print("ä¸ä¸€è‡´ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã™ï¼")
            print(f"å‰å›ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {len(prev_params)}, ä»Šå›ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {len(current_params)}")

            for i, (prev_shape, curr_shape) in enumerate(zip(prev_params, current_params)):
                if prev_shape != curr_shape:
                    print(f"ä¸ä¸€è‡´ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {i+1}, å‰å›: {prev_shape}, ä»Šå›: {curr_shape}")

    @staticmethod
    def compare_model_model_shapes(prev_model, current_model):
        """
        å‰å›ã¨ä»Šå›ã®ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿shapeã‚’æ¯”è¼ƒã—ã€ä¸ä¸€è‡´ã‚’è¡¨ç¤ºã™ã‚‹ã€‚

        Args:
            prev_model (torch.nn.Module): å‰å›ã®ãƒ¢ãƒ‡ãƒ«
            current_model (torch.nn.Module): ä»Šå›ã®ãƒ¢ãƒ‡ãƒ«
        """
        prev_params = {name: param.shape for name, param in prev_model.named_parameters()}
        current_params = {name: param.shape for name, param in current_model.named_parameters()}

        mismatched_layers = []

        print("ã€ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¯”è¼ƒçµæœã€‘")
        print("-" * 50)

        for name, prev_shape in prev_params.items():
            curr_shape = current_params.get(name)
            if curr_shape is None:
                print(f"ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {name}")
            elif prev_shape != curr_shape:
                mismatched_layers.append((name, prev_shape, curr_shape))
                print(f"ä¸ä¸€è‡´ãƒ¬ã‚¤ãƒ¤ãƒ¼: {name}, å‰å›: {prev_shape}, ä»Šå›: {curr_shape}")

        if not mismatched_layers:
            print("å‰å›ã¨ä»Šå›ã®ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ä¸€è‡´ã—ã¦ã„ã¾ã™ï¼ğŸ‰")
        else:
            print("\nã€ä¸ä¸€è‡´ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ä¸€è¦§ã€‘")
            for name, prev_shape, curr_shape in mismatched_layers:
                print(f"ãƒ¬ã‚¤ãƒ¤ãƒ¼å: {name}, å‰å›: {prev_shape}, ä»Šå›: {curr_shape}")
